---
title: "R Notebook"
output: html_notebook
---

# Optimization using the optim() function
Optimization is used in many fields and its goal is to find the best possible value or combination of values with regard to some function, and it is incredibly useful.

There are multiple R functions that can perform optimization. However, this course focuses on the optim() function.

The optim() function in R is a minimization tool. It's goal is to find x in a given function which minimizes f(x).

## Polynomial()
Let us start out by showing an example of one polynomial function. 

```{r}
# We first define our function, in our case it is a polynomial expression. 
polynomial <- function(x){
  f_x <- 10*x^2 + 2*x + 30 
  return(f_x)
}
x_plot <- seq(-10,10, by = .05)
#plot the function as a line (type = "l"). 
plot(x_plot, polynomial(x_plot), type = "l", xlab = 'x', ylab = 'f(x)')
```
**Questions:**
- Using what we know about derivatives how would we find any local maximum and minimum points?

Solution: finding the first order derivative and solving for zero. That is solving $f'(x) = 0$ for $x$.

- When having found an extremum how can you be sure if it's a min or max? (Think in terms of higher order derivatives)

Solution: if $f'(x) = 0$ and $f''(x) > 0$ then $f$ has a local minimum at $x$. Similarly, if $f'(x) = 0$ and $f''(x) < 0$ then $f$ has a local maximum at $x$. In the case where $f'(x) = 0$ and $f''(x) = 0$ then we don't know if it's a min or max.

```{r}
# From the polynomium above
f = expression(10*x^2 + 2*x + 30)

# Second order derivative is positive, thus we know it is a local minimum
D(D(f,'x'),'x')
```

## Using the optim() function
Let's look at how we can find the local minimum using the optim() function in R
```{r}
#check out ?optim()
?optim()
```

We can see that the function takes the following arguments, optim(par = initial values, fn = function to minimize, method = "Which algorithm to use", lower = "lower bound", upper = "upper bound".)


```{r}
#How to find the local minimum using optim function()

#Without derivative method (with bounds)
optim(1, fn = polynomial, method= "Brent", lower = -10, upper = 10)
```
We get several different outputs. \$par indicates the x values which minimized f(x), and \$value indicates what the f(x) values is at the point x which minimized f(x).

```{r}
# with derivative (without bounds)
optim(1, fn = polynomial, method = "CG")
```
We can see that using an algorithm which uses the derivatives is slightly different in its estimates of $x$ which minimize f(x). While it isn't as precise and is more of an estimate it has the advantage of not requiring a lower and upper boundary.

## Sinus function. 
We know that a sinus function has several minimums and maximums so how does the optim() function deal with that? We will follow the same procedure as before, first define the function then try and optimize it. 
```{r}
sin_function <- function(x){
  f_x <- sin(x)
  return(f_x)
}
x <- seq(-10,10 , by = .05)
plot(x,sin_function(x), type = "l")
```

In the definition set of $d_f = (-10:10)$ we can see that there is roughly 3 local minimum. *1)* around x= 5, *2)* around x= -2, *3)* around x= -8. 

```{r}
# optim() will always find the local minimum which is the closest to the starting value.  
x = -10
repeat{
  print(paste("for x = ", x," the closest minimum is at x =",optim(x, sin_function, method = "CG")$par))
  x = x +1
  if (x > 10) break("X limit reached")
}
```
## Using optim for multidimensional optimization

Consider the following multidimensional function
```{r}
f <- function(x1,y1){
 f_x <- (1-x1)^2 + 100*(y1- x1^2)^2
 return(f_x)
}
x <- seq(-2,2,by=.15)
y <- seq(-1,3,by=.15) 
z <- outer(x,y,f) #All possible combination of x,y is used to calculate all possible f(x,y) = z. 
#how to plot 3D
persp(x,y,z,phi=45,theta=-45,col="green",shade=.00000001,ticktype="detailed")
```
When then using the optim() function for multidimensional optimization then the input has to be a multidimensional vector
```{r}
f <- function(x){
  f_x <- (1-x[1])^2 + 100*(x[2]-x[1]^2)^2
  return(f_x)
} 
optim(c(0,0) , f)
```
From the optimization above we can see that the minimum that is closest to (x = 0, y = 0) is around (x = 1, y = 1). Can we be sure that is the global minimum? Not as it currently stands, we could modify our algorithm to look broader or do some weighted search but this is one of the big issues with optimizers. 

## Using optim for RSS on a simple linear regression

We've been introduced to normal lm() linear regression function. But linear regression isn't just linear regression. There exist many different approaches and criteria for which that algorithm should optimize. One approach is the least squares method which tries to minimize error term $\epsilon_i = y_i - (a+b x_i) $. We cannot work with the error term directly since this would require us to know the true estimates of a and b. However, we know the estimates of a and b which we can denote as $\hat{a}$. The goal is therefore to minimize the residuals   $r_i = y_i -(\hat{a} + \hat{b}x_i)$. More precisely the residual sum of squares (RSS). In a machine learning framework we would call the RSS = f(x) our cost-function or loss-function. We wanna minimize our cost/loss when doing regression.

We can now optimize the RSS of a simple regression model with an intercept and 1 predictor. Imagine that the x-axis is the different slope values and the y-axis is the different intercepts and the z-axis is our cost/loss. We now want to find the intercept and slope or in other words the x and y-values which minimizes our RSS or z-axis. All we need to do is create a function which calculates RSS based on our $\theta , X ,y$.


```{r}
set.seed(101) # random seed to reproduce results
n <- 1e2
x <- rnorm(n, 20, 2) # so this is 1e2x1 predictor matrix 
y <- rnorm(n, mean = x *2, sd  =1 )                   # this is our outcome variable, a 1e2x1 vector
X_matrix <- cbind(rep(1, 100), x)      # adding a column of 1s as intercept to the design matrix X (1e2 x 1)
theta <- rep(0,2)               # set up the initial theta 2 x 1 vector
```

```{r}
loss_function <-function(X, y, par){  
  n <- length(y)
  loss <- sum((X%*%par - y)^2)/(n-length(par))
return(loss) 
}
```

```{r}
coef <- optim(par = theta, fn = loss_function , X = X_matrix, y = y, method = "BFGS")
coef$par
```
We now have the two point estimates of our intercept: -1.1967 and slope: 2.057. But we know from methods 1 that there is uncertainty denoted as the SE surrounding these coefficients. The standard error for the slope can for instance be found using
$$
SE\left(b\right)=\sqrt{\frac{1}{n-2} \cdot \frac{\sum\left(y_i-\hat{y}_i\right)^2}{\sum\left(x_i-\bar{x}\right)^2}}
$$

```{r}
SE_beta_calc <- function(X,y,theta){
  n <- length(y)
  x <- X[,2]
  y_hat <- X %*% theta
  
  SE_beta <- ((1/(n-2)) *  (sum((y - y_hat)^2))/sum((x-mean(x))^2))
  return(sqrt(SE_beta))
}
SE_beta_calc(X_matrix, y , coef$par)
```
Using lm() we can see that this yields the same result

```{r}
summary(lm(y~ x))
```
## Exercises on the optim function

1) Choose a mathematical function with e.g. 2-4 minima.

hint 1*

  a) Hard code the function into R and plot it.
```{r}
# We first define our function, in our case it is a polynomial expression. 
polynomial4 <- function(x){
  f_x <- (1/200) * (x+4) * (x+3) * (x+2) * (x+1) * (x-1) * (x-2) * (x-3) * (x-4)
  return(f_x)
}

x_plot <- seq(-10,10, by = .05)
#plot the function as a line (type = "l"). 
plot(x_plot, polynomial4(x_plot), type = "l", ylim = c(-10,10), xlim = c(-5,5), xlab = 'x', ylab = 'f(x)')
abline(h = 0, col = 'blue')
```

  b) Find the 4 minimums using the optim() function. 
```{r}
x = -5
repeat{
  print(paste("for x = ", x," the closest minimum is at x =", optim(x, polynomial4, method = "L-BFGS-B")$par))
  x = x +1
  if (x > 5) break("X limit reached")
}
```
  c) Check if the they are indeed minimums using the second derivative rule we learned last class. 
```{r}
f = expression((1/200) * (x+4) * (x+3) * (x+2) * (x+1) * (x-1) * (x-2) * (x-3) * (x-4))

f_1_d <- D(f,'x') # taking first derivative

f_2_d <- D(f_1_d, 'x') # Taking second derivative


x<-c(3.67868765022314, -1.5025856011253, 1.50258560231549)

eval(f_2_d)
```
d) Find the maximums or in other words, find the x's which maximizes f(x)
1. Find alle maximum via samme approach, som jeg fandt alle minimum tidligere.

```{r}
# We first define our function making the output negative. 
polynomial4neg <- function(x){
  f_x <- -(1/200)*(x+4) * (x+3) * (x+2) * (x+1) * (x-1) * (x-2) * (x-3) * (x-4)
  
  return(f_x)
}

x = -5
repeat{
  print(paste("for x = ", x," the closest maximum is at x =", optim(x, polynomial4neg, method = "L-BFGS-B", lower = -4, upper = 4)$par))
  x = x +1
  if (x > 5) break("X limit reached")
}


x_plot <- seq(-10,10, by = .05)
#plot the function as a line (type = "l"). 
plot(x_plot, polynomial4neg(x_plot), type = "l", ylim = c(-10,10), xlim = c(-5,5), xlab = 'x', ylab = 'f(x)')
abline(h = 0, col = 'blue')

x_plot <- seq(-10,10, by = .05)
#plot the function as a line (type = "l"). 
plot(x_plot, polynomial4(x_plot), type = "l", ylim = c(-10,10), xlim = c(-5,5), xlab = 'x', ylab = 'f(x)')
abline(h = 0, col = 'blue')
```
2) Using the above introduction to the linear regression using optim().
Se video med design matrix

Søg på, hvordan man laver en design matrix i R

find ud af hvad x er. 
  a) Create Nx5 design matrix with the intercept and 4 different predictors. 
```{r}
set.seed(101) # random seed to reproduce results
# Type of Gold
Type <- factor(c(rep("control", times = 4), rep("Mayan", times = 4)))
# Weights of the Gold.
Weight_control <- rnorm(4, mean = 2.5, sd = 1)
Weight_mutant <- rnorm(4, 3, sd = 2)
Weight <- c(Weight_control, Weight_mutant)
# Sizes of the Gold
Size <- rnorm(8, mean = 4, sd = 1)
# Age of the Gold artifact
Age <- rnorm(8, 100, sd = 40)
# Color
Color <- factor(c(rep("Green", times = 2), rep("orange", times = 6)))


DesignMatrix <- model.matrix(~Type+Weight+Age+Color)
```



  b) Simulate y dependent on the design matrix. (Hint: Make y dependent on all the different predictors.) don't forget to add some error. 

```{r}
Type_n <- ifelse(Type == "control", 0, 1)
Color_n <- ifelse(Color == "green", 0, 1)
error <- rnorm(8, 0, 10)

y <- c(Type_n*50+Weight*100+Size*20+Age*(-3)+Color_n*(-10)+error)


```

  c) Create a loss function which we want to minimize (I would suggest RSS or MSE to start with.) 
  
```{r}
theta <- rep(0,5)               # set up the initial theta 5 x 1 vector
loss_function <-function(X, y, par){  
  n <- length(y)
  loss <- sum((X%*%par - y)^2)
return(loss) 
}
```

  d) Use optim() to find the beta coefficients which minimizes our cost function. 
  
```{r}
coef <- optim(par = theta, fn = loss_function , X = DesignMatrix, y = y, method = "BFGS")

coef$par
```


## Exercises from ROS chapter 6

**6.2** Programming fake-data simulation: Write an R function to: (i) simulate n data points from the model, y = a + bx + error, with data points x uniformly sampled from the range (0, 100) and with errors drawn independently from the normal distribution with mean 0 and standard deviation σ; (ii) fit a linear regression to the simulated data; and (iii) make a scatter plot of the data and fitted regression line. Your function should take as arguments, a, b, n, σ, and it should return the data, print out the fitted regression, and make the plot. Check your function by trying it out on some values of a, b, n, σ.
```{r}
library(rstanarm)
library(tidyverse)
```


```{r}
linear_function <- function(a, b, n, sigma) {
  
  if (!require("rstanarm")) {
    library("rstanarm")
  }
  if (!require("tidyverse")) {
    library("tidyverse")
  }
  
  # step 1. simulate n 'x' values that are unifomrly distributed.
  x <- runif(n, min = 0, max = 100)
  # Step 2. Simulate normally distributed error with "sigma" standard deviation
  error <- rnorm(n, mean = 0, sd = sigma)
  # Step 3. Simulate y data points
  y <- a + b*x + error
  # Step 4. make a dataframe
  data <- data.frame(y,x)
  # Step 5. fit a linear regression to the data frame
  model <- stan_glm(y~x, data = data)
  # Step 6 Make a scatterplot of the data with a fitted regression line
  plot_and_line <- plot(x, y, main = "x and y fitted with a regresion line")
  abline(model, col = "red")
  # Step 7 The function returns the dataframe, the fitted regression and the scatterplot
  return(list(data = data, model = model, plot = plot_and_line))
}
```

```{r}

dmp <- linear_function(5, 2, 1000, 5)

```
```{r}
dmp$model
```



**6.3** Variation, uncertainty, and sample size: Repeat the example in Section 6.2, varying the number of data points, n. What happens to the parameter estimates and uncertainties when you increase the number of observations?

hint 3*
More precise parameter estimates

**6.4** (challenging) Simulation study: Perform the previous exercise more systematically, trying out a sequence of values of n, for each simulating fake data and fitting the regression to obtain estimate and uncertainty (median and mad sd) for each parameter. Then plot each of these as a function of n and report on what you find.

```{r}
a <- 5
b <- 2
n <- 10
sigma <- 5
x_median <- c()
x_median_mad_sd <- c()
intercept_mad_sd <- c()
n_amount <- c()
intercept <- c()

for (i in 1:100) {
  # Run function and store the result in a variable
  result <- linear_function(a, b, n, sigma)
  # Extract x_median from the model object
  x_median <- append(x_median, median(result$model$coefficients["x"]))
  x_median_mad_sd <- append(x_median_mad_sd, result$model$ses["x"])
  n_amount <- append(n_amount, n)
  intercept <- c(intercept, result$model$coefficients['(Intercept)'])
  intercept_mad_sd <- c(intercept_mad_sd, result$model$ses['(Intercept)'])
  # n increase by 10 each time
  n <- n + 10
}

df <- tibble(n_amount, x_median, x_median_mad_sd, intercept_mad_sd, intercept)
```

```{r}
# paramter value = 2
plot(df$n_amount, df$x_median, ylim = c(1.96,2.04), xlim = c(5,1000), xlab = 'x', ylab = 'x_median')
abline(h = 2, col = 'blue')
plot(df$n_amount, df$x_median_mad_sd, ylim = c(0.0,0.08), xlim = c(5,1000), xlab = 'x', ylab = 'x_median_mad_sd')
# parameter value = 5
plot(df$n_amount, df$intercept, ylim = c(3.0,9), xlim = c(5,1000), xlab = 'x', ylab = 'intercept')
abline(h = 5, col = 'blue')
plot(df$n_amount, df$intercept_mad_sd, ylim = c(0,3), xlim = c(5,1000), xlab = 'x', ylab = 'intercept_mad_sd')
```

Estimates gets closer parameter values. Mad_sd values for both estimates lower. 


## Exercises from chapter 8

**8.1**
Least squares: The folder ElectionsEconomy contains the data for the example in Section 7.1.
Load these data, type in the R function rss() from page 104, and evaluate it at several different
values of (a, b). Make two graphs: a plot of the sum of squares of residuals as a function of "a",
with "b" fixed at its least squares estimate given in Section 7.1, and a plot of the sum of squares of
residuals as a function of "b", with "a" fixed at its least squares estimate. Confirm that the residual
sum of squares is indeed minimized at the least squares estimate

```{r}
# Loading the data
remotes::install_github("avehtari/ROS-Examples",subdir = "rpackage")
library(rosdata)
```

```{r}
# This is the elections economy data set
df <- hibbs
```




```{r}
# Defining the rss function
rss <- function(x,y,a,b){ # x and y are vectors, a and b are scalars
  resid <- y - (a+b*x)
  return(sum(resid^2))
}

# A plot of the sum of squares of residuals as a function of a, with b fixed at its least squares estimate
y <- hibbs$vote
x <- hibbs$growth
b <- 3

rss_of_a <- c()

a = seq(1, 100)

for (i in a){
 rss_of_a[i] = rss(x,y,i,b)
}

plot(a, rss_of_a, type = 'l', ylab = 'RSS')

```








**8.3**
Least absolute deviation: Repeat 8.1, but instead of calculating and minimizing the sum of
squares of residuals, do this for the sum of absolute values of residuals. Find the (a,b) that
minimizes the sum of absolute values of residuals, and plot the sum of absolute values of residuals
as a function of 0 and of 1. Compare the least squares and least absolute deviation estimates of (a,b).

















hint 1* Think in terms of factorization. If you are completely out of ideas use:  f <- (1/200) * (x+4) * (x+3) * (x+2) * (x+1) * (x-1) * (x-2) * (x-3) * (x-4)

hint 2* Optim() always minimizes the return() so maybe switch the sign? How can max become min?

hint 3* You can use the following commands to extract the coefficient estimates and their standard errors
```{r}
fit_1 <- stan_glm(y ~ x, data=fake)
print('coefficients')
print(fit_1[["coefficients"]])
print('standard errors')
print(fit_1[["ses"]])
```





